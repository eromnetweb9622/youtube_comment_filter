# ==============================
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# ==============================
import os
from dotenv import load_dotenv
load_dotenv()

# ==============================
# YouTube API ë¼ì´ë¸ŒëŸ¬ë¦¬
# ==============================
from googleapiclient.discovery import build

# ==============================
# OpenAI ëŒ“ê¸€ ë¶„ì„ í•¨ìˆ˜
# ==============================
# â— 1ìˆœìœ„ ê°œì„  í¬ì¸íŠ¸:
# - analyze_comment ë‚´ë¶€ GPT í”„ë¡¬í”„íŠ¸ë¥¼
#   "í™•ì‹¤í•  ë•Œë§Œ ìœ„í—˜" ê¸°ì¤€ìœ¼ë¡œ ì™„í™”í•´ì•¼ í•¨
from backend.openai_service import analyze_comment

# ==============================
# YouTube API Key
# ==============================
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# â— API í‚¤ ì—†ì„ ë•Œ ë°”ë¡œ ì—ëŸ¬ í™•ì¸ìš©
if not YOUTUBE_API_KEY:
    raise ValueError("YOUTUBE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")

# ==============================
# YouTube Data API ê°ì²´ ìƒì„±
# ==============================
youtube = build(
    "youtube",
    "v3",
    developerKey=YOUTUBE_API_KEY
)

def get_comments(video_id, max_results=100):
    results = []
    page_token = None

    # ğŸ”¥ ì¹´ìš´í„° 3ê°œë¡œ ë¶„ë¦¬
    normal_count = 0
    abuse_count = 0      # ìš•ì„¤
    spam_count = 0       # ê´‘ê³ /ìŠ¤íŒ¸

    while len(results) < max_results:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=50,
            textFormat="plainText",
            pageToken=page_token
        )
        response = request.execute()

        for item in response.get("items", []):
            snippet = item["snippet"]["topLevelComment"]["snippet"]
            text = snippet["textDisplay"]

            analysis = analyze_comment(text)

            # ==============================
            # ğŸ”¥ category ì •ê·œí™” (3ê°€ì§€ë¡œ ê°•ì œ ë§¤í•‘)
            # ==============================
            raw_category = analysis.get("category", "ì •ìƒ").strip()

            # í—ˆìš©ë˜ëŠ” ê°’ë§Œ í†µê³¼, ë‚˜ë¨¸ì§€ëŠ” ë¬´ì¡°ê±´ ì •ìƒ
            if raw_category in ["ìš•ì„¤", "ê´‘ê³ /ìŠ¤íŒ¸", "ìŠ¤íŒ¸", "ê´‘ê³ "]:
                if "ìš•ì„¤" in raw_category:
                    category = "ìš•ì„¤"
                else:
                    category = "ê´‘ê³ /ìŠ¤íŒ¸"
            else:
                category = "ì •ìƒ"

            # ì¹´ìš´í„° ì¦ê°€
            if category == "ì •ìƒ":
                normal_count += 1
            elif category == "ìš•ì„¤":
                abuse_count += 1
            elif category == "ê´‘ê³ /ìŠ¤íŒ¸":
                spam_count += 1

            results.append({
                "author": snippet["authorDisplayName"],
                "text": text,
                "likeCount": snippet["likeCount"],
                "publishedAt": snippet["publishedAt"],
                "category": category,
                "reason": analysis.get("reason", "ë¶„ì„ ì •ë³´ ì—†ìŒ")
            })

            if len(results) >= max_results:
                break

        page_token = response.get("nextPageToken")
        if not page_token:
            break

    # ==============================
    # ğŸ”¥ ëŒ€ì‹œë³´ë“œê°€ ì›í•˜ëŠ” í˜•íƒœë¡œ summary ë°˜í™˜
    # ==============================
    return {
        "summary": {
            "total": len(results),
            "normal": normal_count,
            "abuse": abuse_count,      # ìš•ì„¤
            "spam": spam_count         # ê´‘ê³ /ìŠ¤íŒ¸
        },
        "comments": results
    }